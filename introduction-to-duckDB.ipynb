{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DuckDB in Python: A Comprehensive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to DuckDB\n",
    "\n",
    "DuckDB is a high-performance, in-memory/in-process analytical database management system designed to execute complex analytical SQL queries fast, efficiently, and reliably over large datasets. It is often referred to as the \"SQLite for analytics\" due to its lightweight nature and ease of integration, making it ideal for analytics tasks, able to run entirely in memory or within an application.\n",
    "\n",
    "It basically means that, DuckDB can process data fast, similar to traditional databases like PostgreSQL or SQLite, but without the need for an external server process. DuckDB is particularly well-suited for data analysis tasks, making it a powerful tool for data scientists and analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why DuckDB?**\n",
    "\n",
    "* **In-Process DB** : DuckDB can be embedded directly into your Python environment, which means you don't need to manage a separate database server.\n",
    "* **Columnar Storage** : It stores data in a columnar format, optimized for analytical queries.\n",
    "* **SQL support** : DuckDB fully supports SQL queries, making it easy to interact with large datasets using well-known SQL syntax.\n",
    "* **Fast and efficient** : DuckDB is designed for speed, particularly for analytical workloads like large aggregations or filtering operations.\n",
    "* **Compatible with Pandas, Parquet, and Arrow** : It supports modern data formats, enabling seamless interaction with other data science libraries.\n",
    "\n",
    "Let's explore how to use DuckDB in Python, going from installation to performing various operations like loading data, querying, and interacting with other Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "To get started with DuckDB in Python, you need to install the DuckDB Python package. You can do this using `pip` or `conda`, depending on your environment:\n",
    "\n",
    "```bash\n",
    "pip install duckdb\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```bash\n",
    "conda install python-duckdb -c conda-forge\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DuckDB Database\n",
    "\n",
    "In DuckDB, databases are either stored as files or kept in memory. For simplicity, let's first work with an **in-memory** database.\n",
    "\n",
    "```python\n",
    "import duckdb as dd\n",
    "\n",
    "# Create an in-memory DuckDB connection\n",
    "con = dd.connect(':memory:')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import duckdb as dd\n",
    "\n",
    "# Create an in-memory DuckDB connection\n",
    "con = dd.connect(':memory:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import duckdb as dd\n",
    "\n",
    "# Create a persistent DuckDB database\n",
    "con = dd.connect('my_database.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as dd\n",
    "\n",
    "# Running a basic SQL query\n",
    "result = dd.sql(\"SELECT 'DuckDB_is_cool' AS answer\").fetchall()\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as dd\n",
    "\n",
    "# Running a basic SQL query\n",
    "result = dd.sql(\"SELECT 'DuckDB_is_cool' AS answer\")\n",
    "print( type(result) )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as db\n",
    "\n",
    "# Create a relation from a SQL query\n",
    "rel = db.sql(\"SELECT * FROM range(10_00) AS tbl(ID)\")\n",
    "\n",
    "# Display the relation\n",
    "rel.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries & Data Ingestion\n",
    "\n",
    "DuckDB supports standard SQL syntax, so you can run any SQL query with ease. Let's start by creating an on-file/persistent database and querying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as dd\n",
    "\n",
    "# Create / connect to database\n",
    "con = dd.connect('my_database.db')\n",
    "con.sql('SHOW ALL TABLES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by creating a table and inserting some data, manually.\n",
    "\n",
    "#### Example 1: Creating a Table and Inserting Data Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table\n",
    "con.execute('''\n",
    "CREATE OR REPLACE TABLE countries (\n",
    "    country VARCHAR,\n",
    "    code VARCHAR,\n",
    "    region VARCHAR,\n",
    "    sub_region VARCHAR,\n",
    "    intermediate_region VARCHAR\n",
    ");\n",
    "''')\n",
    "\n",
    "# Insert some data\n",
    "con.execute('''\n",
    "INSERT INTO countries VALUES\n",
    "('Australia', 'AUS', 'Oceania', 'Australia and New Zealand', ''),\n",
    "('India', 'IND', 'Asia', 'Southern Asia', '');\n",
    "''')\n",
    "\n",
    "con.sql('SHOW ALL TABLES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Creating a Table and Inserting Data Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create second table\n",
    "con.execute('''\n",
    "CREATE OR REPLACE TABLE employees (\n",
    "    id INTEGER,\n",
    "    name VARCHAR,\n",
    "    age INTEGER,\n",
    "    salary DOUBLE\n",
    ");\n",
    "''')\n",
    "\n",
    "# Insert some data in second table\n",
    "con.execute('''\n",
    "INSERT INTO employees VALUES\n",
    "(1, 'Person 1', 30, 70000),\n",
    "(2, 'Person 2', 25, 55000),\n",
    "(3, 'Person 3', 35, 80000);\n",
    "''')\n",
    "\n",
    "\n",
    "# Result of showing tables after creating the second table\n",
    "con.sql('SHOW ALL TABLES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting and Working with Data directly from files\n",
    "\n",
    "In the above examples we saw, DuckDB gives us the capability to create tables and allows us to manually add data to them. However, if we are talking about large sets of data, we can ingest data from a variety of sources, including CSV, Parquet, JSON, etc. files. DuckDB lets us capture and store this data in a database. Let's start by removing the data added manually to the countries table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql('DELETE FROM countries;')\n",
    "con.sql('SELECT * FROM countries;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's insert all the values from the `countires.csv` file directly into the table and see how the data looks, afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql('''\n",
    "        INSERT INTO countries (country, code, region, sub_region, intermediate_region) \n",
    "        (SELECT * FROM \"countries.csv\")\n",
    "''')\n",
    "con.sql('SELECT * FROM countries LIMIT 5;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with persistent data stored in files\n",
    "\n",
    "Once the data is stored in tables as persistent data, we can work with it using standard SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql('''\n",
    "        SELECT  *\n",
    "            FROM\n",
    "                countries\n",
    "            WHERE\n",
    "                region = 'Oceania'\n",
    "                AND sub_region = 'Australia and New Zealand'\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Data directly from files\n",
    "\n",
    "While DuckDB can ingest data from various formats, as discussed above. DuckDB also gives a provision to read from these files into an in-memory DuckDB relation (table) and query them directly, to explore and work with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation = con.sql('''\n",
    "        SELECT  *\n",
    "            FROM\n",
    "                'countries.csv'\n",
    "            WHERE\n",
    "                region = 'Oceania'\n",
    "                AND \"sub-region\" = 'Polynesia'\n",
    "''')\n",
    "relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are known as DuckDB relation objects. We can display all data in these `relations`, as demonstrated above of extract them as a list of tuples using `fetchall` method of these relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(relation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating with Pandas\n",
    "\n",
    "One of DuckDB’s most powerful features is its compatibility with `Pandas`. You can run SQL queries directly on Pandas DataFrames or convert query results into DataFrames.\n",
    "\n",
    "Example: Converting to Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Querying Pandas DataFrames directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'id': [4, 5, 6],\n",
    "    'name': ['Person 4', 'Person 5', 'Person 6'],\n",
    "    'age': [45, 40, 35],\n",
    "    'salary': [100000, 85000, 75000]\n",
    "})\n",
    "\n",
    "con.sql('''\n",
    "        INSERT INTO employees (id, name, age, salary)\n",
    "        SELECT * FROM df\n",
    "''')\n",
    "\n",
    "con.sql('select * from employees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw, DuckDB allows you to run SQL queries directly on a Pandas DataFrame. And as you would have guessed, you can convert query results back into DataFrames using .df()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql('SELECT * FROM df').df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Parquet and Arrow\n",
    "\n",
    "DuckDB also supports efficient handling of Parquet and Arrow formats, commonly used in big data scenarios. You can read data from Parquet files and run SQL queries on them without first loading them into memory.\n",
    "\n",
    "Example: Reading from Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from a Parquet file\n",
    "con.sql(\"SELECT * FROM 'countries.parquet'\").df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, DuckDB integrates well with Apache Arrow and supports operations on Arrow tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Queries and Using DuckDB for Analytical Workloads\n",
    "\n",
    "DuckDB is optimized for performance, especially for analytical queries. DuckDB's architecture, particularly its use of vectorized execution and columnar storage, helps DuckDB to speed up query processing and make it extremely efficient for data analytics. Additionally, DuckDB can operate directly on compressed data formats like Parquet, reducing the need for data decompression.\n",
    "\n",
    "- Window Functions: You can perform windowing operations (e.g., running totals, moving averages).\n",
    "- Group By: Complex group-by operations with large datasets are optimized.\n",
    "- Parallel Execution: DuckDB automatically parallelizes many operations for faster results on large datasets.\n",
    "\n",
    "\n",
    "Example: Group and count countries by their regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql('''\n",
    "        SELECT  region\n",
    "                , COUNT(DISTINCT country) AS country_counts\n",
    "            FROM\n",
    "                countries\n",
    "            GROUP BY\n",
    "                region\n",
    "            ORDER BY\n",
    "                country_counts DESC\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Calculate the average salary and find people with above avg salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "con.sql('''\n",
    "        WITH avg_salary AS (\n",
    "            SELECT\n",
    "                    ROUND(AVG(salary),2) AS avg_salary\n",
    "                FROM\n",
    "                    employees\n",
    "            )\n",
    "        \n",
    "        SELECT\n",
    "                *\n",
    "            FROM\n",
    "                employees\n",
    "            WHERE\n",
    "                salary > (SELECT avg_salary FROM avg_salary)\n",
    "        \n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Comparison: DuckDB vs Pandas\n",
    "\n",
    "DuckDB and Pandas are both popular tools for data manipulation in Python, but they have different strengths. Pandas is a general-purpose data manipulation library optimized for in-memory operations, whereas DuckDB is designed specifically for high-performance, analytical queries on large datasets using SQL. DuckDB's columnar storage and query optimization techniques make it significantly faster than Pandas for complex and large-scale analytical queries.\n",
    "\n",
    "### Why is DuckDB Faster for Analytical Queries?\n",
    "- *Columnar Storage:* DuckDB stores data in a columnar format, which is more efficient for analytical queries (like filtering and aggregations). Pandas stores data in row-major format, which is better for general-purpose operations but can be slower for these specific tasks.\n",
    "\n",
    "- *Query Optimization:* DuckDB uses query optimizations like predicate pushdown, efficient joins, and parallel query execution, which are typically missing or less efficient in Pandas.\n",
    "\n",
    "- *Parallel Execution:* DuckDB can automatically parallelize complex queries, leveraging multiple cores in modern CPUs, while Pandas processes data mostly single-threaded by default.\n",
    "\n",
    "- *On-Disk Storage:* DuckDB efficiently handles datasets that don’t fit into memory by using on-disk storage formats like Parquet, while Pandas requires that all data fits into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Generate a large random dataset\n",
    "data_size = 10**8  # 10 million rows\n",
    "df = pd.DataFrame({\n",
    "    'id': np.arange(data_size),\n",
    "    'value': np.random.randn(data_size)\n",
    "})\n",
    "\n",
    "# Scenario: Filter rows where value is greater than 1\n",
    "\n",
    "# Filtering using Pandas\n",
    "start = time.time()\n",
    "pandas_filtered = df[df['value'] > 1]\n",
    "end = time.time()\n",
    "pandas_time = end - start\n",
    "print(f\"Pandas filtering time: {pandas_time:.2f} seconds\")\n",
    "\n",
    "# Filtering using DuckDB\n",
    "start = time.time()\n",
    "duckdb_filtered = dd.query(\"SELECT * FROM df WHERE value > 1\").df()\n",
    "end = time.time()\n",
    "duckdb_time = end - start\n",
    "print(f\"DuckDB filtering time: {duckdb_time:.2f} seconds\")\n",
    "\n",
    "print(f\"DuckDB is {pandas_time / duckdb_time:.2f} times faster than Pandas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Calculate mean of 'value' column\n",
    "\n",
    "# Aggregating using Pandas\n",
    "start = time.time()\n",
    "pandas_mean = df['value'].mean()\n",
    "end = time.time()\n",
    "pandas_time = end - start\n",
    "print(f\"Pandas filtering time: {pandas_time:.2f} seconds\")\n",
    "\n",
    "# Aggregating using DuckDB\n",
    "start = time.time()\n",
    "duckdb_mean = dd.query(\"SELECT AVG(value) FROM df\").fetchone()[0]\n",
    "end = time.time()\n",
    "duckdb_time = end - start\n",
    "print(f\"DuckDB aggregation time: {duckdb_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "print(f\"DuckDB is {pandas_time / duckdb_time:.2f} times faster than Pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the Connection\n",
    "\n",
    "Once you're done with your queries, always remember to close the DuckDB connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "DuckDB is a powerful tool for performing efficient SQL operations in Python, especially when working with large datasets or complex analytical queries. Its ease of integration with modern data formats like Pandas, Parquet, and Arrow, combined with its fast performance, makes it a valuable addition to any data analyst's toolkit.\n",
    "\n",
    "Whether you're building an in-memory database for fast analytics, working with small datasets or working with large-scale data in Parquet files, DuckDB can simplify the process and accelerate performance. Its SQL syntax is easy to learn, and its compatibility with Python makes it highly flexible for a wide range of data-related tasks.\n",
    "\n",
    "Read more at [DuckDB Python API](https://duckdb.org/docs/api/python/overview):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
